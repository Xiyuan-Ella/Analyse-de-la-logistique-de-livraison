# -*- coding: utf-8 -*-
"""Projet PYTETRE - DA Logistique Ecomm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fatxnKneRYBQnOOlEb_2b-7DgVWL3q_w

# 1) Set Up
"""

#Liste des modules à importer pour le set up

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#Import fichier - Step 
#Instructions : télecharger fichier en local puis choisir le fichier après exécution de cette cellule)
#documentation ici si besoin -> https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92). Fichier ici si besoin -> https://www.kaggle.com/prachi13/customer-analytics

from google.colab import files
uploaded = files.upload()

#Import fichier - Step 2

import io
df1 = pd.read_csv(io.BytesIO(uploaded['Train.csv']))

#Le data set est maintenant dans un Dataframe Pandas



#Test pour vérifier que le fichier est correctement uploadé

df1.head(5)

#Test pour vérifier que le fichier est correctement uploadé

df1.tail()

#Enlever la colonne ID (inutile pour la suite)
df = df1.drop("ID", axis=1)

df.isna().sum()

"""# 2) Visualisations"""

# Analyse descriptive des variables numériques
#création d'un dataframe num_data
num_data = df.select_dtypes(include = ['int64', 'float64'])
#moyenne, médiane, quantiles, min, max
stats = pd.DataFrame(num_data.mean(),columns = ['moyenne'])
stats['median'] = num_data.median()
stats['mean_med_diff'] = abs(stats['moyenne'] - stats['median'])
stats[['q1', 'q2', 'q3']] = num_data.quantile(q = [0.25,0.5,0.75]).transpose()
stats['min'] = num_data.min()
stats['max'] = num_data.max()
stats['min_max_diff'] = stats['max'] - stats['min']
stats['ecart_type'] = num_data.std()
stats.round(2)

#Analyse des variables catégorielles
cat_data = df.select_dtypes(include = "object")
main_category = cat_data.value_counts() 
main_category

cat_data.mode()

#Hypothèse 1 : les commandes arrivent  temps parce que notre entreprise accordent des réductions aux clients
#test statistique entre 'Discount_offered' et 'Reached.on.Time_Y.N'
from scipy.stats import pearsonr
pd.DataFrame(pearsonr( df['Discount_offered'],df['Reached.on.Time_Y.N']), index = ['pearson_coeff','p-value'], columns = ['resultat_test'])
#comme la p-value < 5%, les deux variables sont dépendantes; cependant la corrélation est  modérément positive : r de Pearson = 0,397

#Visualisation 2 : Comparaison entre le Discount et le timing de livraison 
#Rappel : Il y a une corrélation modérée entre "Reached on Time" et "Discount Offered" (0.40)

df_ontime = df[df["Reached.on.Time_Y.N"] == 1]
df_late = df[df["Reached.on.Time_Y.N"] == 0]
np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) 
plt.figure(figsize=[10,5])
plt.hist(x = [df_ontime["Discount_offered"], df_late["Discount_offered"]], bins= [0,10,20,30], label = ["En retard", "A temps"])
plt.xlabel("Discount (in %)")
plt.ylabel("Number of orders")
plt.title("Comparaison entre la présence d'un discount et le timing de livraison")
plt.legend();

#Les produits dont la remise est inférieure à 10 ont une plus grande probabilité d'arriver en retard

#Hypothèse 2 : les remises accordées ont une incidence sur le poids des chaque colis expédié
#test statistique entre 'Discount_offered' et 'Reached.on.Time_Y.N'
from scipy.stats import pearsonr
pd.DataFrame(pearsonr( df['Weight_in_gms'],df['Discount_offered']), index = ['pearson_coeff','p-value'], columns = ['resultat_test'])
#comme la p-value < 5%, les deux variables sont dépendantes; cependant la corrélation est  modérément négative : r de Pearson = -0,376

#Visualisation 3 : Relation entre le poids des produits et le discount (corrélation négative de 0.38)

g = sns.relplot(x="Weight_in_gms", y="Discount_offered", hue='Reached.on.Time_Y.N', data=df)
g.fig.set_figwidth(12)
g.fig.set_figheight(8)
plt.legend(loc='upper right', labels=['A Temps', 'En Retard'])
plt.title("Relation entre le poids, le discount et le timing de livraison");

# Les produits lourds ont une remise faible, cela explique la corrélation négative
# Sur les produits avec un discount, ceux pesants entre 4 et 6 kilos ou moins de 2 kilos & dont la remise est comprise entre 0 et 10% sont ceux arrivant en retard.

#Hypothèse 3 : la relation entre le volume des colis et timing de livraison par entrepôt
#test statistique entre 'Weight_in_gms' et 'Reached.on.Time_Y.N'
from scipy.stats import pearsonr
pd.DataFrame(pearsonr( df['Weight_in_gms'],df['Reached.on.Time_Y.N']), index = ['pearson_coeff','p-value'], columns = ['resultat_test'])
#comme la p-value < 5%, les deux variables sont dépendantes; cependant la corrélation est négative : r de Pearson = -0,002

#Visualisation 4 : Volume de colis et mode d'expédition par entrepôt

order = df[df["Reached.on.Time_Y.N"] == 1].value_counts().index
g = sns.catplot(x="Warehouse_block", kind="count", hue='Reached.on.Time_Y.N', data=df);
g.fig.set_figwidth(12)
g.fig.set_figheight(8)
plt.legend(loc='upper right', labels=['En Retard', 'A Temps'])
plt.title('Volume de colis et timing de livraison par entrepôt');

#L'entrepôt F a un impact important sur le variable cible car il stock un grand nombre de colis

# analyse Warehouse_block et Reached on Time 

df_bis = df.rename(columns={'Reached.on.Time_Y.N': 'Reached'})
import statsmodels.api 

result = statsmodels.formula.api.ols('Reached ~ Warehouse_block', data=df_bis).fit()
table = statsmodels.api.stats.anova_lm(result)

table

#Hypothese 4 :le coût des produits a une influence sur le nombre d'appels clients
#test statistique entre 'Customer_care_calls' et ''
from scipy.stats import pearsonr
pd.DataFrame(pearsonr( df['Customer_care_calls'],df['Cost_of_the_Product']), index = ['pearson_coeff','p-value'], columns = ['resultat_test'])
#comme la p-value < 5%, les deux variables sont dépendantes; la corrélation est proche de 0

#Visualisation 5 : Dispersion du coût des produits par nombre de customer calls
#Il y a une corrélation faible (0.32)entre les "Customer Care Calls" et le "Cost of The Product"
g = sns.catplot(x = "Customer_care_calls", y = "Cost_of_the_Product",kind = "box", data = df)
g.fig.set_figwidth(6)
g.fig.set_figheight(7)
plt.title("Dispersion du coût des produits par nombre d'appels au service client");

#Hypothèse : plus le produit est cher, plus le client va demander des informations sur le suivi des expéditions.

#Hypothèse 5:  l'impact du genre sur la note client, le nombre d'appel et le nombre d'achats
#test statistique entre 'Gender' et 'Customer_rating'
import statsmodels.api 
result = statsmodels.formula.api.ols('Customer_rating~Gender', data = df).fit()
table = statsmodels.api.stats.anova_lm(result)
table
#comme la p-value >5%, les deux variables sont indépendantes

#test statistique entre 'Gender' et 'Customer_care_calls'
import statsmodels.api 
result = statsmodels.formula.api.ols('Customer_care_calls~Gender', data = df).fit()
table = statsmodels.api.stats.anova_lm(result)
table
#comme la p-value >5%, les deux variables sont indépendantes

#test statistique entre 'Gender' et 'prior_purchase'
import statsmodels.api 
result = statsmodels.formula.api.ols('Prior_purchases~Gender', data = df).fit()
table = statsmodels.api.stats.anova_lm(result)
table
#comme la p-value >5%, les deux variables sont indépendantes

#Visualisation 6 : Impact du genre sur la note client, le nombre d'appel et le nombre d'achats

j = 1
var = ['Customer_rating','Customer_care_calls','Prior_purchases']
plt.figure(figsize = (15,5))
for i in var:
    plt.subplot(1,3,j)
    sns.countplot(x = i , data = df, hue= 'Gender')
    j +=1
plt.suptitle("Impact du genre sur la note client, le nombre d'appels et le nombre d'achats");

# Nous observons que les chiffres pour les deux sexes sont à peu près égaux.

#Synthèse des corrélationsentre les différentes variables 
#Visualisation 1 : Corrélation entre les variables

cor=df.corr()
fig,ax = plt.subplots(figsize=(10,10))
fig = sns.heatmap(cor, annot=True, ax=ax, cmap = "coolwarm");
plt.title("Matrice de corrélation");

# Comme on peut le voir sur le graphique ci-dessous, les trois groupes suivants sont en corrélation :
#1) "Reached on Time" VS "Discount Offered" corrélation modéré (0.40)
#2) "Customer Care Calls" VS  "Cost of The Product" corrélation faible (0.32)
#3) "Weight of The Product in Grams" VS "Discount Offered" corrélation faible négative(-0.38)

#Autre visualisation : analyse du volume de colis par moyen de transport
plt.figure(figsize=[12,6])
ship_order=df['Mode_of_Shipment'].value_counts().index
sns.countplot(data=df,x='Mode_of_Shipment',order=ship_order)
plt.title('Volume de colis par moyen de transport');

#Suite à la visualisation des différentes variables retenues explicatives en fonction de la variable à expliquer nous pouvons supprimer les valeurs supérieures à 1.5 fois la hauteur de la boîte à moustache.
df = df[df['Weight_in_gms'] > 2500]
df = df[df['Prior_purchases']< 6.5]
df.head()

"""#3) Preprocessing

"""

df.info()

#On vérifie qu'il n'y a pas de valeurs manquantes

df.isna().sum()

#On affiche le nom des colonnes

df.columns

#On sépare en varaibles catégorielles et numériques. On met la variable target de côté pour faciliter l'exécution de la cellule suivante.

categorical = ['Warehouse_block','Mode_of_Shipment','Customer_rating', 'Product_importance', 'Gender']
numeric = ['Customer_care_calls', 'Cost_of_the_Product', 'Prior_purchases', 'Discount_offered', 'Weight_in_gms']
target = ['Reached.on.Time_Y.N']

#On encode les valeurs catégorielles
Dummy_encoded_feats = pd.get_dummies(df, columns= categorical)

#On encode notre valeur target. On l'encode à part pour avoir 1 seule colonne (avec 1 si le colis est arrivé à temps et 0 si il est en retard).
Dummy_encoded_target = pd.get_dummies(df["Reached.on.Time_Y.N"], drop_first = True)

#On drop la colonne target du dataframe feats
Dummy_encoded_feats = Dummy_encoded_feats.drop(columns=['Reached.on.Time_Y.N'])

#On vérifie que le code s'est bien exécuté
Dummy_encoded_target.value_counts()
Dummy_encoded_target.head(5)
Dummy_encoded_feats.head(5)

#On fusionne les 2 dataframes dummy encoded
df_dummy_encoded = pd.concat([Dummy_encoded_feats, Dummy_encoded_target], axis = 1)

#On renomme la colonne target avec un nom plus clair
df_dummy_encoded.rename({1: 'On_Time?'}, axis=1, inplace=True)

#On vérifie que le code s'ext bien exécuté
df_dummy_encoded.head(5)
#C'est le cas, notre dataframe df_dummy_encoded est prêt pour le processing !

df_dummy_encoded.info()

"""# 4) Processing - Standard - Evaluation des modèles


"""

#Import des modules nécessaires

from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import confusion_matrix 
from sklearn import preprocessing

target = df_dummy_encoded['On_Time?']
data = df_dummy_encoded.iloc[:,0:-1]
#On sépare target / data

scaler = preprocessing.StandardScaler()
df = data.select_dtypes(include="int64") 
#On recupère les 5 premières colonnes pour appliquer un StandardScaler afin de normaliser 
#Opération réalisée sur les 5 premières colonnes uniquement car les autres colonnes contiennent uniquement des 0 ou des 1
dfnum = pd.DataFrame(scaler.fit_transform(df),index=df.index, columns = df.columns) 
#On applique Standardscaler
dfnum.head(5)

dfcat = data.select_dtypes(include="uint8") 
#On recupère le reste des colonnes

data = dfnum.join(dfcat) 
#On join le deux dataframes
data.head(5)

X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=66)
#On sépare en groupe train/test

Models = {
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "Logistic Regression": LogisticRegression(),
    "KNN": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier()
}
#On crée un dictionnaire avec tous les modèles

for name, model in Models.items():
    model.fit(X_train, y_train)
    print(name, ":", model.score(X_test, y_test))
#On affiche le score de chaque modèle
#Gradient Boosting semble être le modèle le plus performant

"""#5) Processing - Avancé - Decision Tree Models

Decision Tree model (criterion = 'entropy')
"""

dt_clf = DecisionTreeClassifier(criterion='entropy',max_depth=4,random_state=123)
dt_clf.fit(X_train, y_train)
y_pred_dt = dt_clf.predict(X_test)
pd.crosstab(y_test, y_pred_dt, rownames = ['Classes réelles'], colnames = ['Classes prédites'])

impVar={"Variable":data.columns,"Importance":dt_clf.feature_importances_}
print(pd.DataFrame(impVar).sort_values(by="Importance",ascending=False).head(8))

from sklearn.datasets import load_iris
from sklearn import tree
iris = load_iris()
clf = dt_clf.fit(iris.data, iris.target)
tree.plot_tree(clf)

"""Decision Tree model (criterion = 'gini')"""

dt_clf_gini = DecisionTreeClassifier(criterion='gini',max_depth=4,random_state=321)
dt_clf_gini.fit(X_train,y_train)
y_pred = dt_clf_gini.predict(X_test)
pd.crosstab(y_test, y_pred, rownames=['Classes réelles'], colnames = ['Classes prédites'])

impVar_gini={"Variable":data.columns,"Importance":dt_clf_gini.feature_importances_}
print(pd.DataFrame(impVar_gini).sort_values(by="Importance",ascending=False).head(8))

from sklearn.datasets import load_iris
from sklearn import tree
iris = load_iris()
clf = dt_clf_gini.fit(iris.data, iris.target)
tree.plot_tree(clf)

"""#6) Processing - Avancé - Pipelines"""

#On importe les modules nécessaires

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler

#On isole la variable target
df = df1
target = df['Reached.on.Time_Y.N']
data = df.drop('Reached.on.Time_Y.N', axis=1)

#On sépare en variables ordinales, nominales, numériques

ordinal = [
    'Gender',
    'Product_importance'
]

nominal = [
    'Warehouse_block',
    'Mode_of_Shipment'
]
#Rappel : "nominal" data can only be classified, "ordinal" data can be classified and ordered, "numerical" data is a number that can be measured

numerical = [
    'Cost_of_the_Product',
    'Discount_offered',
    'Weight_in_gms',
    'Prior_purchases',
    'Customer_care_calls',
    'Customer_rating'
]

#On crée les pipelines, on utilise 3 encoders différents pour chaque type de données

transformer_for_ordinal = Pipeline([
    ('ordinal', OrdinalEncoder())
])

transformer_for_nominal = Pipeline([
    ('nominal', OneHotEncoder())
])

transformer_for_numerical = Pipeline([
    ('numerical', MinMaxScaler())
])

#On crée le Transformer

Transformer = ColumnTransformer(transformers=[
    ('ordinal', transformer_for_ordinal, ordinal),
    ('nominal', transformer_for_nominal, nominal),
    ('numerical', transformer_for_numerical, numerical)
])

#On applique les transformations sur les data.

data_transform = pd.DataFrame(Transformer.fit_transform(data))
data_transform

#On sépare en groupe train/test
X_train, X_test, y_train, y_test = train_test_split(data_transform, target, test_size=0.2, random_state=66)

Models = {
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "Logistic Regression": LogisticRegression(),
    "KNN": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier()
}
#On crée un dictionnaire avec tous les modèles

for name, model in Models.items():
    model.fit(X_train, y_train)
    print(name, ":", model.score(X_test, y_test))
#On affiche le score de chaque modèle
#Gradient Boosting semble toujours être le modèle le plus performant (score 0,67), cependant le score du modèle est meilleur avec ce processing (score 0,66 avec le processing classique).

"""# 7) Interprétabilité du modèle Gradient Boosting

Code à lancer avant lancement du Processing - Avancé - Pipelines
"""

from sklearn import ensemble
clf = ensemble.GradientBoostingClassifier()
clf.fit(X_train, y_train)

features = data.columns
feature_importance = clf.feature_importances_
sorted_idx = np.argsort(feature_importance)
pos = np.arange(sorted_idx.shape[0]) + .5
fig = plt.figure(figsize=(10,10))
plt.barh(pos, feature_importance[sorted_idx], align='center')
plt.xticks(fontsize=8, rotation=90)
plt.yticks(pos, features[sorted_idx])
plt.xlabel('Relative Importance')
plt.title('Variable Importance')
plt.show()

"""# Appendix-Brouillons (visualisations et processing)

Appendix) Brouillons visualisations
"""

#Visualisation 1 : Usage des différents modes d'expédition
plt.figure(figsize=[7,4])
sns.countplot(data=df, x="Mode_of_Shipment", order = df['Mode_of_Shipment'].value_counts().index)
plt.title("Usage des différents modes d'expédition");

#Visualisation 2 : Usage en % des modes d'expédition
plt.figure(figsize=[7,4])
df['Mode_of_Shipment'].value_counts().plot(kind='pie', fontsize=12,autopct='%1.1f%%', startangle=0, shadow = True, explode= (0.1, 0,0))
plt.title("% d'usage des modes d'expédition");

# Visualisation 3 : Réponse à la question : Les commandes sont-elles livrées à temps ?
ax = sns.barplot(x="Reached.on.Time_Y.N", y='Reached.on.Time_Y.N', data=df, estimator=lambda x: len(x) / len(df) * 100)
ax.set(ylabel="% des commandes")
plt.title("Timing de livraison")
plt.xticks([0,1],['En retard','A temps']);

#Autres visualisations possibles
#Visualisation 4
plt.figure(figsize = (12,12))
plt.subplot(321)
df['Reached.on.Time_Y.N'].value_counts(normalize=True).plot(kind='pie',autopct='%1.1f%%')
plt.title("Répartition des commandes arrivées à temps")
plt.subplot(322)
sns.countplot(x = "Reached.on.Time_Y.N", hue = "Mode_of_Shipment", data = df)
plt.title("Répartition des commandes arrivées à temps selon le mode de transport")
plt.subplot(322)
sns.catplot(x = "Reached.on.Time_Y.N", y = "Customer_rating",kind = "box", data = df)
plt.title("Répartition des commandes arrivées à temps selon la notation client");

#Traitement des valeurs aberrantes ?

#Ideas
#Relation Between Weight of The Product in Grams - Discount Offered
#Which Mode of Shipment is Faster Than Others?
#What is The Number of Times The Products Stored In Each Warehouse Arrive on Time?
#Which Gender is More Satisfied With The Orders?

#Visualisations To Do
#1) "Customer Care Calls" VS  "Cost of The Product" corrélation faible (0.32) -> Boites à Moustache (x nombre d'appels / y coût) : Thierry (Done)
#2) "Reached on Time" VS "Discount Offered" corrélation modérée (0.40) -> Histogramme (TBC) : Vivien (Done)
#3) Visualisations pour repérer valeurs aberrantes : Corine (Done)
#4) "Weight_in_gms" VS "Discount_offered" corrélation négatif (-0.38):  Xiyuan

#Visualisation 5 : Comparaison entre le Discount et le timing de livraison 
#Rappel : Il y a une corrélation modérée entre "Reached on Time" et "Discount Offered" (0.40)
# Les produits dont la remise est inférieure à 10 ont une plus grande probabilité d'arriver
df_ontime = df[df["Reached.on.Time_Y.N"] == 1]
df_late = df[df["Reached.on.Time_Y.N"] == 0]

plt.figure(figsize=[10,5])
plt.hist(x = [df_ontime["Discount_offered"], df_late["Discount_offered"]], bins= [0,10,20,40], label = ["A temps","En retard"])
plt.xlabel("Discount (en %)")
plt.ylabel("Nombre de commandes")
plt.title("Comparaison entre la présence d'un discount et le timing de livraison")
plt.legend();

#Visualisation 6 : Comparaison entre le Discount et le timing de livraison (en %)

df_discount = df[df["Discount_offered"] > 1]
df_nodiscount = df[df["Discount_offered"] == 1]

plt.figure(figsize=[5,10])

plt.subplot(2,1,1)
df_discount['Reached.on.Time_Y.N'].value_counts().plot(kind='pie', fontsize=12,autopct='%1.1f%%', startangle=0, labels = ["A temps", "En retard"])
plt.title("Timing de livraison des commandes AVEC discount")
plt.legend();

plt.subplot(2,1,2)
df_nodiscount['Reached.on.Time_Y.N'].value_counts().plot(kind='pie', fontsize=12, autopct='%1.1f%%', startangle=0, labels = ["A temps", "En retard"])
plt.title("Timing de livraison des commandes SANS discount")
plt.legend();

#  Visualisation 7 : Relation "Weight_in_gms" VS "Discount_offered" corrélation négatif (-0.38)
fig, ax =plt.subplots(figsize=(10, 5))
sns.scatterplot(x="Discount_offered", y="Weight_in_gms", hue='Reached.on.Time_Y.N',
                data=df, ax=ax);

# remiese jusqu'à 10% sur les produits de plus de 4 kg
# plus le produit est lourd, plus la remise est faible, cela explique la corrélation néfatif
# 6 produis exceptionnelles

# resumer: les produits a été livré à temps lorsque la remise est inférieure à 10% et que le poids est de 1-2kg ou 4-6kg,
# les produits pesant entre 2 et 4kg et les produits dont la remise est supérieure à 10% n'arrivent pas à temps.

# Visualisation 9: la plupart des colis sont stockés dans l'entrepôt F
# la livraison de cet entrepôt a un impact important sur le variable cible
sns.catplot(x="Warehouse_block", kind="count", hue='Mode_of_Shipment',data=df);

#Dès lors nous pouvons analyser les corrélationsentre les variables
#Visualisation 10 : Corrélation entre les variables
cor=df.corr()
fig,ax = plt.subplots(figsize=(10,10))
sns.heatmap(cor, annot=True, ax=ax, cmap = "coolwarm");
# Comme on peut le voir sur le graphique ci-dessous, les trois groupes suivants sont en corrélation :
#"Reached on Time" VS "Discount Offered" corrélation modéré (0.40)
#"Customer Care Calls" VS  "Cost of The Product" corrélation faible (0.32)
#"Weight of The Product in Grams" VS "Discount Offered" corrélation faible négative(-0.38)

#Visualisations To Do
#1) "Customer Care Calls" VS  "Cost of The Product" corrélation faible (0.32) -> Boites à Moustache (x nombre d'appels / y coût) : Thierry

#Visualisation 11 : j'émets l'hypothèse que plus le produit est cher plus le client va demander des informations sur le suivi des expéditions
sns.catplot(x = "Customer_care_calls", y = "Cost_of_the_Product",kind = "box", data = df)
plt.title("Dispersion du coût des produits par nombre de customer calls");
#hypothèse confirmée

# Visualisation 12 : j'émets l'hypothèse que le warehouse de stockage a un impact sur la livraison dans les temps des produits
sns.catplot(x="Warehouse_block", kind="count", hue='Reached.on.Time_Y.N',data=df);
# Hypothèse non confirmée : la proportion semble la même quel que soit l'entrepôt

#Visualisation 13 :  j'émets l'hypothèse que la plupart des produits importants sont transportés par avion
sns.countplot(df['Mode_of_Shipment'], hue=df['Product_importance']);

# Faux, la pluspart des produits sont expédiés par voie maritime, y compris la plupart des produits importants

# visualisation 14: j'émets l'hypothese que le gender n'a pas d'importance pour le note de client, nombre d'appel et nombres d'achats
j = 1
var = ['Customer_rating','Customer_care_calls','Prior_purchases']
plt.figure(figsize = (15,5))
for i in var:
    plt.subplot(1,3,j)
    sns.countplot(x = i , data = df, hue= 'Gender')
    j +=1

# Nous observons que les chiffres pour les deux sexes sont à peu près égaux.
# Vérifié l'hypothese

#visualisation 15: 
ct_counts=df.groupby(['Product_importance','Reached.on.Time_Y.N']).size()
ct_counts=ct_counts.reset_index(name='count')
ct_counts=ct_counts.pivot(index='Product_importance',columns='Reached.on.Time_Y.N',values='count')
sns.heatmap(ct_counts,annot=True,fmt='d');
plt.title('On time shiping rate per each product quality',fontsize=16);

print('High quality product rate:',round(394/(394+279),2)*100,'%')
print('Low quality product rate:',round(1688/(1688+1493),2)*100,'%')
print('med quality product rate:',round(1629/(1639+1453),2)*100,'%')
# Les produits de haute qualité ont le meilleur taux d'expédition dans les délais.

#repérer les valeurs aberrantes ==> la boîte à moustache
#sur les boites à moustaches sont représentés la médiane, les quartiles 25% et 75% ainsi les valeurs 
#les plus extrêmes dans la limite de 1.5 fois la hauteur de la boîte.
#Comment repérer les valeurs aberrantes ?
#Les valeurs qui sont juste au-dessus des boîtes à moustaches ne sont pas considérées 
#comme des valeurs aberrantes mais plutôt comme des valeurs extrêmes.
#Ces valeurs ne semblent pas relever une anomalie dans notre jeu de données à 
#linverse de celles qui se trouvent particulièrement eloignées des autres, celles qui sont isolées.
#Ces dernieres seront considérées comme aberrantes. 
#liste des variables à représenter
groupe_variables = ['Warehouse_block', 'Mode_of_Shipment', 'Customer_care_calls',
       'Customer_rating', 'Cost_of_the_Product', 'Prior_purchases',
       'Product_importance', 'Gender', 'Discount_offered', 'Weight_in_gms']
for i in groupe_variables:
    fig, axs = plt.subplots(1, 1, figsize=(15, 5))
    sns.boxplot(y=df[i], x=df['Reached.on.Time_Y.N'], ax=axs, color='#16E4CA')
    plt.title(i)

#Suite à la visualisation des différentes variables retenues explicatives en fonction de la variable à expliquer
#nous pouvons supprimer les valeurs supérieures à 1.5 fois la hauteur de la boîte à moustache
df = df[df['Weight_in_gms'] > 2500]
df = df[df['Prior_purchases']< 6.5]
df.head()

#Vérification des doublons
df.duplicated().sum()
#Conclusion : nous n'avons pas de doublons dans notre dataset

#Suite aux diférentes visualisations efectuées il apparaît que les variables agissant sur l'arrivée en temps et en heure sont principalement : le prix du produit, le nombre d'appel des clients (par ricochet du prix du produit), le discount offert et le poids de l'expédition.
#Les autres variables n'agissent à priori pas sur le résultat.

"""Appendix) Brouillons processing"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=66)

#Classificaiton par régression logistique
from sklearn.metrics import confusion_matrix 
from sklearn import linear_model
clf = linear_model.LogisticRegression(C=1.0)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
cm = pd.crosstab(y_test, y_pred, rownames=['Classe réelle'], colnames=['Classe prédite']) 
print(cm)
print('score de la régression logistique est: ',clf.score(X_test, y_test) )

# Random Forest
from sklearn import ensemble
from sklearn.model_selection import train_test_split
clf = ensemble.RandomForestClassifier(n_jobs=-1, random_state=321)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
pd.crosstab(y_test, y_pred, rownames=['Classe réelle'], colnames=['Classe prédite'])
print('score de Random Forest est: ',clf.score(X_test, y_test))

from sklearn.model_selection import train_test_split
data = df_dummy_encoded.iloc[:,7:-1]
target = df_dummy_encoded['On_Time?']

data.head(5)

"""KNN"""

# création d'une instance clfknn avec un paramétrage au "hasard" et apprentissage
from sklearn import neighbors
clfknn = neighbors.KNeighborsClassifier(n_neighbors=10)
clfknn.fit(X_train,y_train)

# fixation des paramètres 
parametresknn =[{"n_neighbors":list(range(2,50))}]

# recherche des hyperparamètres retenus 
from sklearn import model_selection
grid_clfknn = model_selection.GridSearchCV(estimator=clfknn, param_grid=parametresknn)
grid_clfknn.fit(X_train, y_train)
print(grid_clfknn.best_params_)

# application du modèle à l'ensemble de test et affichage de la matrice de confusion
y_pred = grid_clfknn.predict(X_test)
pd.crosstab(y_test, y_pred, rownames=['Classe réelle'], colnames=['Classe prédite'])

# affichage du score du modèle
print(grid_clfknn.best_score_) 
#Le best score trouvé est meileur que le score de départ en comparaison des modèles.

# KNN avec hyperparamètres

from sklearn import neighbors
from sklearn import model_selection
knn = neighbors.KNeighborsClassifier()
parametres = {'n_neighbors': range(2,50)}
grid_knn = model_selection.GridSearchCV(estimator=knn, param_grid=parametres)

grid_knn.fit(X_train, y_train)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA


df_ACP=df.replace(['A','B','C','D','F'],[0,1,2,3,4])
df_ACP=df_ACP.replace(['Flight','Ship','Road'],[0,1,2])
df_ACP=df_ACP.replace(['low', 'medium', 'high'],[0,1,2])
df_ACP=df_ACP.replace(['F', 'M'],[0,1])

scaler = StandardScaler()
Z = scaler.fit_transform(df_ACP)

pca=PCA()
Coord = pca.fit_transform(Z)
print('Les ratio sont :',pca.explained_variance_ratio_)

plt.plot(np.arange(1, 12), np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Factor number')
plt.ylabel('Cumsum');

print('Les valeurs propres sont :', pca.explained_variance_) # pour chaque composantes de PCA

plt.plot(np.arange(1,12), pca.explained_variance_)
plt.xlabel('nombre de facteur')
plt.ylabel('explained variance');

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2,random_state=66)

dt_clf = DecisionTreeClassifier()
dt_clf.fit(X_train, y_train)
y_pred_dt = dt_clf.predict(X_test)
print('score:',dt_clf.score(X_test, y_test))
pd.crosstab(y_test, y_pred_dt, rownames = ['Classes rélles'], colnames = ['Classes prédites'])

data_test = X_test.join(y_test)
data_test = data_test.reset_index()
list = pd.DataFrame(y_pred_dt)
data_test_pred = data_test.join(list)
data_test_pred.rename({0: 'predict'}, axis=1, inplace=True)

df_bis = df1.drop("ID", axis=1)
df_bis = df_bis.reset_index()

data_test_pred_bis = data_test_pred[['index', 'On_Time?', 'predict']]

data_test_pred_bis = data_test_pred_bis.merge(df_bis, on = 'index', how = 'left')

good_predict = data_test_pred_bis[data_test_pred_bis["On_Time?"] == data_test_pred_bis["predict"]]
bad_predict = data_test_pred_bis[data_test_pred_bis["On_Time?"] != data_test_pred_bis["predict"]]

## Warehouse_block
bad_w = bad_predict['Warehouse_block'].value_counts(normalize = True)
good_w = good_predict['Warehouse_block'].value_counts(normalize = True)
print(bad_w)
print(good_w)

## Mode_of_Shipment
bad_s = bad_predict['Mode_of_Shipment'].value_counts(normalize = True)
good_s = good_predict['Mode_of_Shipment'].value_counts(normalize = True)
print(bad_s)
print(good_s)

## Customer_care_calls
bad_cc = bad_predict['Customer_care_calls'].value_counts(normalize = True)
good_cc = good_predict['Customer_care_calls'].value_counts(normalize = True)
print(bad_cc)
print(good_cc)

## Prior_purchases
bad_pp = bad_predict['Prior_purchases'].value_counts(normalize = True)
good_pp = good_predict['Prior_purchases'].value_counts(normalize = True)
print(bad_pp)
print(good_pp)

## Product_importance
bad_pi = bad_predict['Product_importance'].value_counts(normalize = True)
good_pi = good_predict['Product_importance'].value_counts(normalize = True)
print(bad_pi)
print(good_pi)

## customer_rating
bad_pi = bad_predict['Customer_rating'].value_counts(normalize = True)
good_pi = good_predict['Customer_rating'].value_counts(normalize = True)
print(bad_pi)
print(good_pi)

## exploration de données
# datavis 2
num_data = df.select_dtypes(include=['int64','float64'])
num_data_vis2= pd.DataFrame( num_data, columns = ['Discount_offered'])
num_data_vis2.head()
num_data_vis3 = pd.DataFrame( num_data, columns = ['Discount_offered','Weight_in_gms'])
num_data_vis3

stats = pd.DataFrame(num_data_vis2.mean(),columns=['moyenne'])

stats['mediane']=num_data_vis2.median()
stats['mean_med_diff'] = abs(stats['moyenne']-stats['mediane'])
stats[['q1','q2','q3']] = num_data_vis2.quantile(q=[0.25,0.5,0.75]).transpose()
stats['min'] = num_data_vis2.min()
stats['max'] = num_data_vis2.max()
stats['min_max_diff'] = stats['max'] - stats['min']
stats.round(2)

stats = pd.DataFrame(num_data_vis3.mean(),columns=['moyenne'])

stats['mediane']=num_data_vis3.median()
stats['mean_med_diff'] = abs(stats['moyenne']-stats['mediane'])
stats[['q1','q2','q3']] = num_data_vis3.quantile(q=[0.25,0.5,0.75]).transpose()
stats['min'] = num_data_vis3.min()
stats['max'] = num_data_vis3.max()
stats['min_max_diff'] = stats['max'] - stats['min']
stats.round(2)





